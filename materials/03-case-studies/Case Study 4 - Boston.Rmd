---
title: "Case Study 4: Boston -- Scalar Regression to predict Boston House Prices"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Initialize package
# install.packages(keras)
library(keras)
library(ggplot2)
```

In our last case study, Boston, we'll perform a regression to predict a continouous response variable from 13 predictor variables. To accommodate for this different analytical problem, we'll use:

- A new normalization for the input data, z scores,
- A new loss function, `mse`,
- A new metric, `mae`, and
- No final activation function (i.e. scalar).

And since we have a really small data set we'll have:

- A very simple network architecture, and
- K-fold crossvalidation.

## Install tensorflow 

It's only necessary to run this once. 

```{r install, eval = F}
# for GPU
# install_keras(tensorflow = "gpu")

# or CPU:
# install_keras() # for cpu
```

# Part 1: Data Preparation

## Obtain data

```{r data, warning = FALSE}
c(c(train_data, train_targets), c(test_data, test_targets)) %<-% dataset_boston_housing()
```

## Examine data:

Our predictor variables:

```{r strDataPre}
str(train_data)
str(test_data)
```

The target, response variable:

```{r strTargets}
str(train_targets)
```

## Prepare the data:

Convert z-scores:

$$z_i=\frac{x_i-\bar{x}}{s}$$

```{r zScores, cache = T}
# parameters for Scaling:
mean <- colMeans(train_data) # mean of each column
std <- apply(train_data, 2, sd) # stdev of each column

# Calculate feature-wise (within-variable) z-scores: (x - mean)/std
train_data <- scale(train_data, center = mean, scale = std)
test_data <- scale(test_data, center = mean, scale = std)
```

# Part 2: Define Network

## Define the network as a function

In contrast to our previous case studies, we're going to call the same model multiple times. So we'll create a function with no arguments that we can call to create our model when ever we want to use it for training. 

Here, I've hardcoded the number of features for this dataset (`13`). To generalize, we could just use `dim(train_data)[2]` to get the number of dimensions from the training set.  

```{r defModel}
build_model <- function() {
  network <- keras_model_sequential() %>% 
    layer_dense(units = 64, activation = "relu", input_shape = 13) %>% 
    layer_dense(units = 64, activation = "relu") %>% 
    layer_dense(units = 1) 
    
  network %>% compile(
    optimizer = "rmsprop", 
    loss = "mse", 
    metrics = c("mae")
  )
}
```

Note two new functions here, the mean squared error:

$$\operatorname{MSE} = \frac{\sum_{i=1}^n(y_i-\hat{y_i})^2}{n} = \frac{\sum_{i=1}^n{e_i^2}}{n}$$
and the mean absolute error (MAE):

$$\mathrm{MAE} = \frac{\sum_{i=1}^n\left| y_i-\hat{y_i}\right|}{n} = \frac{\sum_{i=1}^n\left| e_i\right|}{n}$$
where $\hat{y_i}$ is the predicted value, given in our last single-unit layer, and $y_i$ is the actual value, the label.

# Part 3: k-fold cross validation

```{r setkFold, echo = TRUE, results = 'hide'}
k <- 4 # four groups
indices <- sample(1:nrow(train_data)) # randomize the training set before splitting for k-fold cross validation:
folds <- cut(indices, breaks = k, labels = FALSE) # divide the ordered indices into k intervals, labelled 1:k.
```

```{r kfold100, cache = T}
num_epochs <- 100
all_scores <- c() # An empty vector to store the results from evaluation

for (i in 1:k) {
  cat("processing fold #", i, "\n")
  # Prepare the validation data: data from partition # k
  val_indices <- which(folds == i, arr.ind = TRUE) 
  
  # validation set: the ith partition
  val_data <- train_data[val_indices,]
  val_targets <- train_targets[val_indices]
  
  # Training set: all other partitions
  partial_train_data <- train_data[-val_indices,]
  partial_train_targets <- train_targets[-val_indices]
  
  # Call our model function (see above)
  network <- build_model()
  
  # summary(model)
  # Train the model (in silent mode, verbose=0)
  network %>% fit(partial_train_data,
                  partial_train_targets,
                  epochs = num_epochs,
                  batch_size = 1,
                  verbose = 0)
                
  # Evaluate the model on the validation data
  results <- network %>% evaluate(val_data, val_targets, verbose = 0)
  all_scores <- c(all_scores, results$mean_absolute_error)
}  
```

We get 4 mae values

```{r allscores}
all_scores
```

### Training for 500 epochs

Let's try training the network for a bit longer: 500 epochs. To keep a record of how well the model did at each epoch, we will modify our training loop to save the per-epoch validation score log:

```{r clearMem}
# Some memory clean-up
K <- backend()
K$clear_session()
```

Train our the models:

```{r kfold500, echo = T, results = 'hide', cache = T}
num_epochs <- 500
all_mae_histories <- NULL # an empty object to cumulatively store the model metrics

for (i in 1:k) {
  cat("processing fold #", i, "\n")
  
  # Prepare the validation data: data from partition # k
  val_indices <- which(folds == i, arr.ind = TRUE)
  val_data <- train_data[val_indices,]
  val_targets <- train_targets[val_indices]
  
  # Prepare the training data: data from all other partitions
  partial_train_data <- train_data[-val_indices,]
  partial_train_targets <- train_targets[-val_indices]
  
  # Build the Keras model (already compiled)
  model <- build_model()
  
  # Train the model (in silent mode, verbose=0)
  history <- model %>% fit(partial_train_data, 
                           partial_train_targets,
                           validation_data = list(val_data, val_targets),
                           epochs = num_epochs, 
                           batch_size = 1, 
                           verbose = 0
  )
  mae_history <- history$metrics$val_mean_absolute_error
  all_mae_histories <- rbind(all_mae_histories, mae_history)
}
```

Calculate the average per-epoch MAE score for all folds:

```{r plot1}
average_mae_history <- data.frame(
  epoch = seq(1:ncol(all_mae_histories)),
  validation_mae = apply(all_mae_histories, 2, mean)
)

p <- ggplot(average_mae_history, aes(x = epoch, y = validation_mae))

p + 
  geom_point()

p + 
  geom_smooth(method = 'loess', se = FALSE)
```

According to this plot, it seems that validation MAE stops improving significantly after circa 80 epochs. Past that point, we start overfitting.

Once we are done tuning other parameters of our model (besides the number of epochs, we could also adjust the size of the hidden layers), we can train a final "production" model on all of the training data, with the best parameters, then look at its performance on the test data:

```{r runZ, echo = F, results = 'hide', cache = T}
# Get a fresh, compiled model.
model <- build_model()

# Train it on the entirety of the data.
model %>% fit(train_data, 
              train_targets,
              epochs = 80, 
              batch_size = 16, 
              verbose = 0)

result <- model %>% evaluate(test_data, test_targets)
```

```{r resultsZ}
result
```

We are still off by about `r round(result$mean_absolute_error * 1000)`.

## Alternatives: No Normalization

Let's imagine that we didn't normalize the input variables

```{r setupNone, echo = F, cache = T}
# Obtain the raw data
c(c(train_data, train_targets), c(test_data, test_targets)) %<-% dataset_boston_housing()

# Some memory clean-up
K <- backend()
K$clear_session()

num_epochs <- 500
all_mae_histories <- NULL # an empty object to cumulatively store the model metrics

for (i in 1:k) {
  cat("processing fold #", i, "\n")
  
  # Prepare the validation data: data from partition # k
  val_indices <- which(folds == i, arr.ind = TRUE)
  val_data <- train_data[val_indices,]
  val_targets <- train_targets[val_indices]
  
  # Prepare the training data: data from all other partitions
  partial_train_data <- train_data[-val_indices,]
  partial_train_targets <- train_targets[-val_indices]
  
  # Build the Keras model (already compiled)
  model <- build_model()
  
  # Train the model (in silent mode, verbose=0)
  history <- model %>% fit(partial_train_data, 
                           partial_train_targets,
                           validation_data = list(val_data, val_targets),
                           epochs = num_epochs, 
                           batch_size = 1, 
                           verbose = 0
  )
  mae_history <- history$metrics$val_mean_absolute_error
  all_mae_histories <- rbind(all_mae_histories, mae_history)
}
```

Calculate the average per-epoch MAE score for all folds:

```{r plot2, echo = F, cache = T}
average_mae_history <- data.frame(
  epoch = seq(1:ncol(all_mae_histories)),
  validation_mae = apply(all_mae_histories, 2, mean)
)

p <- ggplot(average_mae_history, aes(x = epoch, y = validation_mae))

p + 
  geom_point()

p + 
  geom_smooth(method = 'loess', se = FALSE)
```

The validation MAE stops improving significantly after circa 140 epochs.

```{r runNone, echo = F, results = 'hide', cache = T}
# Get a fresh, compiled model.
model <- build_model()

# Train it on the entirety of the data.
model %>% fit(train_data, 
              train_targets,
              epochs = 140, 
              batch_size = 16, 
              verbose = 0)

result_none <- model %>% evaluate(test_data, test_targets)
```

```{r resultsNone}
result_none
```

Now, without any normalization, we're off by about `r round(result_none$mean_absolute_error * 1000)`, compared to `r round(result$mean_absolute_error * 1000)` previously.

## Alternatives: 0-1 normalization 

How about if we did 0-1 normalization?

$$z_i=\frac{x_i-\min(x)}{\max(x)-\min(x)}$$

```{r setup01, echo = F, cache = T}
# Obtain the raw data
c(c(train_data, train_targets), c(test_data, test_targets)) %<-% dataset_boston_housing()

train_data <- apply(train_data, 2, function(x) (x-min(x))/(max(x)-min(x)))
test_data <- apply(test_data, 2, function(x) (x-min(x))/(max(x)-min(x)))

# Some memory clean-up
K <- backend()
K$clear_session()

num_epochs <- 500
all_mae_histories <- NULL # an empty object to cumulatively store the model metrics

for (i in 1:k) {
  cat("processing fold #", i, "\n")
  
  # Prepare the validation data: data from partition # k
  val_indices <- which(folds == i, arr.ind = TRUE)
  val_data <- train_data[val_indices,]
  val_targets <- train_targets[val_indices]
  
  # Prepare the training data: data from all other partitions
  partial_train_data <- train_data[-val_indices,]
  partial_train_targets <- train_targets[-val_indices]
  
  # Build the Keras model (already compiled)
  model <- build_model()
  
  # Train the model (in silent mode, verbose=0)
  history <- model %>% fit(partial_train_data, 
                           partial_train_targets,
                           validation_data = list(val_data, val_targets),
                           epochs = num_epochs, 
                           batch_size = 1, 
                           verbose = 0
  )
  mae_history <- history$metrics$val_mean_absolute_error
  all_mae_histories <- rbind(all_mae_histories, mae_history)
}
```

Calculate the average per-epoch MAE score for all folds:

```{r plot3, echo = F, cache = T}
average_mae_history <- data.frame(
  epoch = seq(1:ncol(all_mae_histories)),
  validation_mae = apply(all_mae_histories, 2, mean)
)

p <- ggplot(average_mae_history, aes(x = epoch, y = validation_mae))

p + 
  geom_point()

p + 
  geom_smooth(method = 'loess', se = FALSE)
```

The validation MAE stops improving significantly after circa 140 epochs.

```{r run01, echo = F, results = 'hide', cache = T}
# Get a fresh, compiled model.
model <- build_model()

# Train it on the entirety of the data.
model %>% fit(train_data, 
              train_targets,
              epochs = 140, 
              batch_size = 16, 
              verbose = 0)

result_01 <- model %>% evaluate(test_data, test_targets)
```

```{r results01}
result_01
```

Here, we're off by about `r round(result_01$mean_absolute_error * 1000)`, compared to `r round(result$mean_absolute_error * 1000)` with z scores.
