---
title: "Computer vision & CNNs: Transfer Learning"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

# Initialize package
library(keras)
```

In this module, we are going to use a pretrained CNN model to perform image 
classification on our dogs vs. cats images. 

Learning objectives:

- Why using pretrained models can be efficient and effective.
- How to perform feature extraction with pretrained models.
- How you can fine tune a pretrained model and run end-to-end.

# Part 1: Data Preparation

We are working with the same dogs and cats images as before.

## Image location

```{r image-file-paths}
# define the directories:
image_dir <- here::here("docs", "data", "dogs-vs-cats")
train_dir <- file.path(image_dir, "train")
valid_dir <- file.path(image_dir, "validation")
test_dir <- file.path(image_dir, "test")
```

# Applying a pretrained CNN model

There are ___two main ways___ we can apply a pretrained model to perform a CNN:

1. Feature extraction
2. Fine tune a pretrained model and run end-to-end

##  Option 1: Feature extraction

### Get our pretrained model

There are several pretrained models available via keras; they all start with 
`application_`. Here we'll use the VGG16 model; it is intuitive to understand 
the model structure and it does a good job with this task.

- `weights`: Represents the weights to use. Most pretrained models are built on 
  imagenet and using these weights tends to do well.
- `include_top`: Whether to include the fully-connected dense classifier. Typically, 
  we want the classifier to be specific to our problem.
- `input_shape`: The shape of our nputs (150x150 pixel images w/3 color channels).

```{r pretrained-model}
conv_base <- application_vgg16(
  weights = "imagenet",
  include_top = FALSE,
  input_shape = c(150, 150, 3)
)

```

```{r vgg16-model-structure}
conv_base
```

### Extracting features using the pretrained convolutional base

This seems a little daunting to understand but this is just implementing more of 
a manual approach to what you already have been doing. Here we create a function 
that will:

1. Create an empty tensor to hold our transformed features and labels.
2. Create our data batch importing generator. Here we will use batch size of 20 
   simply because we have 2000 training images and 1000 validation images and 
   a batch size of 20 divides nicely to both of these values.
3. Loop through our training data:
   a. import a batch of images
   b. apply the pretrained base to our images to output the predicted features
   c. add these new features and the labels to our tensor
   d. continue to do this until our number of iterations x batch size equals or 
      exceeds our total number of samples.
      
**Without a GPU this will take approximately 5 minutes to execute**        

```{r image-generator-feature-extraction, include=FALSE}

datagen <- image_data_generator(rescale = 1/255)
batch_size <- 20

extract_features <- function(directory, sample_count) {
  features <- array(0, dim = c(sample_count, 4, 4, 512))   # step 1
  labels <- array(0, dim = c(sample_count))                # step 1
  generator <- flow_images_from_directory(                 # step 2
    directory = directory,                                 
    generator = datagen,                                   
    target_size = c(150, 150),
    batch_size = batch_size,
    class_mode = "binary"
  )
  i <- 0
  while (TRUE) {                                           # step 3
    cat("Processing batch", i + 1, "of", ceiling(sample_count / batch_size), "\n")
    batch <- generator_next(generator)                     # step 3a
    inputs_batch <- batch[[1]]
    labels_batch <- batch[[2]]
    features_batch <- conv_base %>% predict(inputs_batch)  # step 3b
    index_range <- ((i * batch_size) + 1):((i + 1) * batch_size)
    features[index_range,,,] <- features_batch             # step 3c
    labels[index_range] <- labels_batch                    # step 3c
    i <- i + 1
    if (i * batch_size >= sample_count) break              # step 3d
    }
  list(
    features = features,
    labels = labels
  ) }


train <- extract_features(train_dir, 2000)
validation <- extract_features(valid_dir, 1000)
test <- extract_features(test_dir, 1000)
```

### Reshape features

The extracted features will be a 4D tensor (samples, 4, 4, 512).  We can see this 
in the last layer of our `conv_base` model above (`block5_pool (MaxPooling2D)`). 

Consequently, we need to reshape (flatten) these into a 2D tensor to feed into 
a densely connected classifier. This results in a 2D tensor of size 
(samples, 4 * 4 * 512 = 8192).

```{r reshape-features}
reshape_features <- function(features) {
  array_reshape(features, dim = c(nrow(features), 4 * 4 * 512))
}
train$features <- reshape_features(train$features)
validation$features <- reshape_features(validation$features)
test$features <- reshape_features(test$features)

dim(train$features)
```

### Define densely connected classifier model

Here we only need the densely connected classifier.

```{r model-classifier}

model <- keras_model_sequential() %>%
  layer_dense(units = 256, activation = "relu", input_shape = 4 * 4 * 512) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 1, activation = "sigmoid")

model

```

Compile and train:

This will train quickly, taking approximately 1 min when trained on your local 
CPU. Our validation loss also improves over the previous CNN we built.

```{r train-model, include=FALSE}

model %>% compile(
  optimizer = optimizer_rmsprop(lr = 2e-5),
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history1 <- model %>% fit(
  train$features, train$labels,
  epochs = 30,
  batch_size = 32,
  validation_data = list(validation$features, validation$labels)
)

```

# Plot results:

```{r plot-model1-history}
plot(history1)
```

So we acheived a significant decrease in our loss score, increased our accuracy 
to 90%, and did so in a fraction of the time!


# ------------------------- ONLY RUN ON GPU!! ----------------------------------

The above approach performed pretty well. However, we can see that we are still 
overfitting, which may be reducing model performance. An alternative approach is 
to run a pretrained model from end-to-end. This approach is much slower and 
computationally intense; however, it offers greater flexibility in using and 
adjusting the pretrained model because it lets you:

1. use data augmentation to decrease overfitting (and usually increase model 
   performance).
2. fine tune parts of the pretrained model.

##  Option 2a: Run end-to-end with data augmentation

The following approach simply plugs the pretrained convolution base into a 
sequential model but freezes the convolution base weights.

### Combining a densely-connected neural network with the convolutional base

In this case we can literally plug in our `conv_base` within our model 
architecture. 

**To run the code chunks in this section change `eval=TRUE`**

```{r CNN-base-and-classifier}

model <- keras_model_sequential() %>%
  conv_base %>%
  layer_flatten() %>%
  layer_dense(units = 256, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

model
```

### Freezing

Before you compile and train the model, it's important to freeze the convolutional 
base weights. This prevents the weights from being updated during training. If 
you don't do this then the representations found in the pretrained model will be 
modified and, potentially, completely destroyed.

```{r freeze-parameters}
cat(length(model$trainable_weights), "trainable weight tensors before freezing.\n")

freeze_weights(conv_base)

cat(length(model$trainable_weights), "trainable weight tensors before freezing.\n")
```

#### Training the model end-to-end with a frozen convolutional base

The following trains the model end-to-end using all CNN logic that you have seen 
before:

1. data augmentation
2. image generator
3. compile our model
4. train our model

**To run this code chunk change `eval=TRUE`**

```{r train-end-to-end, eval=FALSE}

train_datagen = image_data_generator(
  rescale = 1/255,
  rotation_range = 40,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.2,
  horizontal_flip = TRUE,
  fill_mode = "nearest"
)

test_datagen <- image_data_generator(rescale = 1/255)

train_generator <- flow_images_from_directory(
  train_dir,
  train_datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)

validation_generator <- flow_images_from_directory(
  validation_dir,
  test_datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)

model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 1e-5),
  metrics = c("accuracy")
)

history2 <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 100,
  epochs = 30,
  validation_data = validation_generator,
  validation_steps = 50
)

```

#### View history

```{r plot-model2-history}
plot(history2)
```


##  Option 2a: Fine tune Run end-to-end

Another widely used technique for using pretrained models, is to unfreeze a few 
of the convolutional base and allow those weights to be updated. Recall that 
the early layers in a CNN identify detailed edges and shapes. Later layers put 
these edges and shapes together to make higher order parts of the images we are 
trying to classify (i.e. cat ears, dog tails). 

The more our images deviate from the images used to create the pretrained model, 
then the more likely you will want to retrain the last few layers, which will 
make the edge and shape features more relevant to your problem.

To fine-tune a pretrained model you:

1. Add your custom network on top of an already-trained base network (executed 
   in the `CNN-base-and-classifier` code chunk).
2. Freeze the base network (executed in the `freeze-parameters` code chunk).
3. Train the part you added (executed in the `train-end-to-end` code chunk).
4. Unfreeze some layers in the base network.
5. Jointly train both these layers and the part you added.

We already did steps 1-3. The following executes steps 4 and 5.

### Unfreeze some layers in the base network

```{r unfreeze-some-layers}
unfreeze_weights(conv_base, from = "block3_conv1")
```


### Jointly  train both these layers and the part you added

**To run this code chunk change `eval=TRUE`**

```{r fine-tune-model, eval=FALSE}
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 1e-5),
  metrics = c("accuracy")
)

history2 <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 100,
  epochs = 100,
  validation_data = validation_generator,
  validation_steps = 50
)
```

