---
title: "The 'Hello World' of Deep Learning"
output: html_notebook
---

# Welcome to your first deep learning module!

This module is designed to provide you an introduction to deep learning and 
some of the key components that make DL algorithms run. Throughout this case 
study you will learn:

* What tensors are
* What makes up a basic feedforward neural network architecture
* What forward and backward passes are
* How a neural network learns via backpropagation and batch processing

In doing so you will run your first neural network with one of the most famous 
benchmark data sets --> MNIST.

# Installation

Install and load the `keras` package.

```{r setup}
knitr::opts_chunk$set(echo = TRUE)

# Install package from CRAN if necessary
# install.packages('keras')

library(keras)     # for deep learning
library(dplyr)     # for minor data wrangling
library(ggplot2)   # for plotting
```

Once the package is installed, you need to install the Keras and TensorFlow 
Python packages, which is what the R Keras and TensorFlow packages communicate
with. 

* both GPU & CPU options are available
* can install in a virtual or conda environment
* can setup for Theano & CNTK backends rather than TensorFlow

This installation can be quite simple, and it can get quite complex depending on
your current system setup and needs. 

For this workshop we will be using a cloud environment to ensure we are all 
operating in common environment and Keras and TensorFlow have already been 
installed.

```{r install, eval = FALSE}
# default CPU-based installations of Keras and TensorFlow
# install_keras()

# for GPU capabilities
# install_keras(tensorflow = "gpu")
```


# Part 1: Data Preparation

## Obtain data

`keras` has many built in data sets (or functions to automatically install data
sets). Check out what data is available with `dataset_` + tab.

We're going to use the ___MNIST___ data set which is the "hello world" for learning
deep learning!

```{r data}
mnist <- dataset_mnist()
str(mnist)
```

When we work with keras:

* training and test sets need to be independent
* features and labels (aka target, response) need to be independent
* use `%<-%` for ___object unpacking___ (see `?zeallot::%<-%`)

```{r extract-train-test}
c(c(train_images, train_labels), c(test_images, test_labels)) %<-% mnist
```

## Data structure

Our training images (aka features) are stored as a 3D array

* 60,000 images consisting of a...
* 28x28 matrix with...
* values ranging from 0-255 representing gray scale pixel values.

```{r features}
dim(train_images)
```

Check out the first digit

```{r first-digit}
digit <- train_images[1,,]
digit
```

Lets plot the first digit and compare to the above matrix

```{r plot-first-digit}
plot(as.raster(digit, max = 255))
```

Now lets check out the first 100 digits

```{r plot-first-100-digits}
par(mfrow = c(10, 10), mar = c(0,0,0,0))
for (i in 1:100) {
  plot(as.raster(train_images[i,,], max = 255))
}
```

## Reshape data:

Our current data structure is incompatible for modeling a basic neural network

```{r current-structure-features}
str(train_images)
str(test_images)
```

We need to reshape these into a 2D ___tensor___

```{r reshape-to-2D-tensor}
train_images <- array_reshape(train_images, c(60000, 28 * 28))
test_images <- array_reshape(test_images, c(10000, 28 * 28))

str(train_images)
str(test_images)
```


## Normalize data:

Our feature values range from 0-255. For our purposes we want to normalize these
values to be between 0-1. We'll discuss why this is important later.

```{r normalize-feature-values}
train_images <- train_images / 255
test_images <- test_images / 255
```


## Prepare labels:

Our response data are currently 1D arrays (60,000 vectors of length 1) with the
actual digit the image represents.

```{r current-structure-labels}
str(train_labels)
str(test_labels)
```

We could use that structure for our response but, often, for classification 
problems we'll reformat with `to_categorical()`:

```{r reshape-labels}
train_labels <- to_categorical(train_labels)
test_labels <- to_categorical(test_labels)

head(train_labels)
```


# Part 2: Training a DL model

Nearly all DL models have a very similar training process:

1. Define the network architecture
2. Define network compilation
3. Execute training loop


## 1. Define the network architecture

```{r architecture}
network <- keras_model_sequential() %>%
  layer_dense(units = 512, activation = 'relu', input_shape = ncol(train_images)) %>%
  layer_dense(units = 10, activation = 'softmax')
```

You can view a summary of the network

```{r summary}
summary(network)
```

## 2. Define network compilation

```{r compile}
network %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "rmsprop",
  metrics = c("accuracy")
)
```

## 3. Execute training loop

```{r train, include=FALSE}
history <- network %>% 
  fit(train_images, train_labels, 
      batch_size = 128, epochs = 20, 
      validation_split = 0.2)
```

When training the model in RStudio, you will see a real-time plot of the loss
metric along with any other metrics identified to track.

You can also `plot` this trained model object, which just uses `ggplot2` and 
can be modified like any other `ggplot2` plot (as long as `ggplot2` is loaded.

```{r plotTraining}
plot(history) + ggtitle("My first deep learning model")
```

# YOUR TURN!

1. Identify the optimal number of epochs from our previous model
2. Fill in the blanks below and re-run the model

```{r optimal-model}
network <- keras_model_sequential() %>%
  layer_dense(units = 512, activation = _____, input_shape = ncol(train_images)) %>%
  layer_dense(units = 10, activation = _____)

network %>% compile(
  optimizer = "rmsprop",
  loss = _____,
  metrics = c("accuracy")
)

# use optimal number of epochs
history <- network %>% 
  fit(train_images, train_labels, 
      batch_size = 128, epochs = _____, 
      validation_split = 0.2)
```


# Part 3: Evaluate results

## Metrics

```{r metrics}
metrics <- network %>% evaluate(test_images, test_labels)
metrics

# Test set accuracy rate
metrics$acc

# Test set error rate
1 - metrics$acc

```

## Predictions

You can predict either the probability of each class. Note that column 1 is for 
digit 0, column 2 is for digit 1, ..., column 10 is for digit 9.

```{r predict-probabilities}
network %>% predict_proba(test_images[1:10,])
```

Or you can predict the class

```{r predict-classes}
network %>% predict_classes(test_images[1:10,])
```

Lets get all our predictions and the actual responses for the test set.

```{r predictions-vs-actuals}
predictions <- network %>% predict_classes(test_images)
actual <- mnist$test$y
```

# Confusion Matrix

We can see how many missed predictions our model had.

```{r missed-predictions}
missed_predictions <- sum(predictions != actual)
missed_predictions
```

We can see what digits our model confuses the most:

* 9s are often confused with 4s
* 8s are often confused with 2s & 3s
* etc.

```{r confusion-matrix}
caret::confusionMatrix(factor(predictions), factor(actual))
```

We can also visualize this with the following:

```{r visual-confusion-matrix}
data.frame(target = mnist$test$y,
                      prediction = network %>%
                        predict_classes(test_images)) %>% 
  filter(target != prediction) %>% 
  group_by(target, prediction) %>%
  count() %>%
  ungroup() %>%
  mutate(perc = n/nrow(.)*100) %>% 
  filter(n > 1) %>% 
  ggplot(aes(target, prediction, size = n)) +
  geom_point(shape = 15, col = "#9F92C6") +
  scale_x_continuous("Actual Target", breaks = 0:9) +
  scale_y_continuous("Prediction", breaks = 0:9) +
  scale_size_area(breaks = c(2,5,10,15), max_size = 5) +
  coord_fixed() +
  ggtitle(paste(missed_predictions, "mismatches")) +
  theme_classic() +
  theme(rect = element_blank(),
        axis.line = element_blank(),
        axis.text = element_text(colour = "black")) +
  labs(caption = 'Courtesy Rick Scavetta')

```


# Visualize missed predictions

Lastly, lets check out those mis-predicted digits.

```{r, mis-predicted-digits}
missed <- which(predictions != actual)
plot_dim <- ceiling(sqrt(length(missed)))

par(mfrow = c(plot_dim, plot_dim), mar = c(0,0,0,0))
for (i in missed) {
  plot(as.raster(mnist$test$x[i,,], max = 255))
}
```

If we look at the predicted vs actual we can reason about why our model 
mispredicted some of the digits.

```{r}
par(mfrow = c(4, 4), mar = c(0,0,2,0))

for (i in missed[1:16]) {
  plot(as.raster(mnist$test$x[i,,], max = 255)) 
  title(main = paste("Predicted:", predictions[i]))
}
```